


# Install the titlecase library
!pip install titlecase



!pip install faker tqdm


pip install fastcluster


pip install langdetect fuzzywuzzy[speedup] python-Levenshtein



pip install joblib


!pip install scikit-surprise


pip install matplotlib


pip install seaborn


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

import os
from scipy.sparse import coo_matrix





from google.colab import drive
drive.mount('/content/drive')


dataset_data= pd.read_csv('/content/drive/MyDrive/book-rec-collab/NEW.csv')


dataset_data.head(2)


dataset_data.isnull().sum()


dataset_data.duplicated().sum()


dataset_data.drop_duplicates(inplace=True)


dataset_data.columns


column_name_mapping = {

    'User':'userID',
    'profileName' : 'userName',
    'ISBN' : 'bookID',
    'authors_split': 'authors',
    'categories_split':'category_list'
}

dataset_data.rename(columns=column_name_mapping, inplace=True)


dataset_data.columns





dataset_data['bookTitle'] = dataset_data['Title']
dataset_data['bookAuthor'] = dataset_data['authors']
dataset_data['bookCategory'] = dataset_data['category_list']
dataset_data['bookPublisher']=dataset_data['publisher']


dataset_data['bookTitle']=dataset_data['bookTitle'].astype(str)
dataset_data['bookAuthor']=dataset_data['bookAuthor'].astype(str)
dataset_data['bookCategory']=dataset_data['bookCategory'].astype(str)
dataset_data['bookPublisher']=dataset_data['bookPublisher'].astype(str)


from titlecase import titlecase

# Apply the titlecase function to the 'title' column
dataset_data['bookTitle'] = dataset_data['bookTitle'].apply(titlecase)



dataset_data['bookAuthor'] = dataset_data['bookAuthor'].apply(titlecase)
dataset_data['bookCategory'] = dataset_data['bookCategory'].apply(titlecase)
dataset_data['bookPublisher'] = dataset_data['bookPublisher'].apply(titlecase)





# remove special characters from book title
import string
dataset_data['Title'] = dataset_data['Title'].str.translate(str.maketrans('', '', string.punctuation))



#rim whitespace
dataset_data['Title'] = dataset_data['Title'].str.strip()



#remove common words like "a", "an"
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))
dataset_data['Title'] = dataset_data['Title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))


# lemmatization/ stemming to reduce words to thir base form
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')
dataset_data['dataset_data'] = dataset_data['Title'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))






# Identify and handle empty processed titles
empty_titles = dataset_data[dataset_data['Title'] == '']
print("Number of titles reduced to empty strings:", len(empty_titles))
# Decide to remove or retain these entries
dataset_data = dataset_data[dataset_data['Title'] != '']





records = pd.read_csv('/content/records.csv')


empty_title_rows = records[dataset_data['Title'] == '']


num_empty_titles = len(empty_title_rows)
total_titles = len(records)
print(f"Number of titles reduced to empty strings: {num_empty_titles} out of {total_titles}")


from fuzzywuzzy import fuzz
from fuzzywuzzy import process



def find_matches(row, choices, threshold=80):
    match = process.extractOne(row['Title'], choices, scorer=fuzz.token_sort_ratio)
    if match[1] >= threshold:
        return match[0]
    else:
        return row['Title']
unique_titles = records['Title'].unique()
records['canonical_title'] = records.apply(find_matches, axis=1, choices=unique_titles)



records.columns


records['canonical_title'].nunique()


#cluster titles
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(records['canonical_title'])

k = 50  #number of clusters
model = KMeans(n_clusters=k)
model.fit(X)

records['cluster'] = model.labels_



#create canonical_book_id
records['canonical_book_id'] = records.groupby('canonical_title').ngroup()


copied = records.copy()


dataset_data=copied.copy()


book_meta = records.groupby('canonical_book_id').agg({
    'userID': 'first',
    'userName': 'first',
    'rating': 'first',
    'bookTitle': 'first',
    'bookAuthor': 'first',
    'bookCategory': 'first',
    'bookPublisher': 'first',
    'description': 'first',
    'image': 'first',
    'rating': 'first',
    'Title': 'first',
    'authors': 'first',
    'publishedDate': 'first',
    'Price': 'first',
    'publisher': 'first',
    'categories': 'first',
    # Add other fields as necessary
}).reset_index()



#chceck duplicates for same user rating for same book with canonical_book_id
duplicates = records[records.duplicated(subset=['userID','userName', 'canonical_book_id','authors','Price','publisher'], keep=False)]



#resolve duplicates
records = records.groupby(['userID', 'canonical_book_id']).agg({
    'userName': 'first',
    'bookID': 'first',
    'rating': 'first',
    'bookTitle': 'first',
    'bookAuthor': 'first',
    'bookCategory': 'first',
    'bookPublisher': 'first',
    'description': 'first',
    'image': 'first',
    'rating': 'first',
    'Title': 'first',
    'authors': 'first',
    'publishedDate': 'first',
    'Price': 'first',
    'publisher': 'first',
    'categories': 'first',
}).reset_index()


records.to_csv('data.csv', index=False)





# Number of unique bookIDs
num_bookIDs = records['bookID'].nunique()

# Number of unique canonical_book_ids
num_canonical_book_ids = records['canonical_book_id'].nunique()

print(f"Number of unique bookIDs: {num_bookIDs}")
print(f"Number of unique canonical_book_ids: {num_canonical_book_ids}")



records.nunique()





rec = records.copy()



# Filter users with at least 5 ratings
user_rating_counts = records.groupby('userID').size().reset_index(name='rating_count')
active_users = user_rating_counts[user_rating_counts['rating_count'] >= 5]['userID']
df_active_users = records[records['userID'].isin(active_users)]

# Identify top N books
book_rating_counts = df_active_users.groupby('canonical_book_id').size().reset_index(name='rating_count')
top_books = book_rating_counts.sort_values(by='rating_count', ascending=False)

# Adjust N to approach 10,000 records
N = 500
top_N_books = top_books.head(N)['canonical_book_id']
df_top_books = df_active_users[df_active_users['canonical_book_id'].isin(top_N_books)]

# Check the number of records
total_records = df_top_books.shape[0]
print(f"Total records after filtering: {total_records}")








# Sample
if total_records > 10000:
    df_sampled = df_top_books.sample(n=10000, random_state=42)
else:
    df_sampled = df_top_books







from faker import Faker
from tqdm import tqdm
import datetime

# Initialize Faker and set seed for reproducibility
fake = Faker()
Faker.seed(42)

# Assuming 'df' is your DataFrame
num_rows = df_sampled.shape[0]

# Define date range
start_date = datetime.date(2014, 1, 1)
end_date = datetime.date(2019, 12, 31)

# Generate random dates using date_between
timestamps = [fake.date_between(start_date=start_date, end_date=end_date) for _ in tqdm(range(num_rows), desc="Generating timestamps")]

# Assign the timestamps to the DataFrame
df_sampled['timestamp'] = timestamps



# Save the reduced dataset
df_sampled.to_csv('reduced_dataset.csv', index=False)


df_sampled=pd.read_csv('/content/drive/MyDrive/IRWA/reduced_dataset.csv')




#Calculate number of ratings per book
book_rating_counts = df_sampled.groupby('canonical_book_id').size().reset_index(name='rating_count')

#Merge with the original dataset
df = pd.merge(df_sampled, book_rating_counts, on='canonical_book_id', how='left')

#Calculate number of ratings per user
user_rating_counts = df_sampled.groupby('userID').size().reset_index(name='rated_books_count')

#Merge with the original dataset
df = pd.merge(df, user_rating_counts, on='userID', how='left')



df.head(2)





# basic statics
print(df['rating'].describe())


num_users = df['userID'].nunique()
num_books = df['canonical_book_id'].nunique()
print(f"Number of unique users: {num_users}")
print(f"Number of unique books: {num_books}")



import matplotlib.pyplot as plt

df['rating'].hist(bins=5, edgecolor='black')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.show()



#Top 10 books with most ratings
top_books = df.groupby('canonical_book_id').size().reset_index(name='rating_count')
top_books = top_books.sort_values(by='rating_count', ascending=False).head(8)
top_books = pd.merge(top_books, df[['canonical_book_id', 'bookTitle']].drop_duplicates(), on='canonical_book_id', how='left')
print(top_books[['bookTitle', 'rating_count']])



top_books.plot(kind='barh', x='bookTitle', y='rating_count', legend=False)
plt.title('Top 10 Most Rated Books')
plt.xlabel('Number of Ratings')
plt.ylabel('Book Title')
plt.gca().invert_yaxis()
plt.show()



#top 10 most engaged users
top_users = df.groupby('userID').size().reset_index(name='rating_count')
top_users = top_users.sort_values(by='rating_count', ascending=False).head(10)
print(top_users)


top_users.plot(kind='barh', x='userID', y='rating_count', legend=False)
plt.title('Top 10 Most Active Users')
plt.xlabel('Number of Ratings Given')
plt.ylabel('User ID')
plt.gca().invert_yaxis()
plt.show()



# rating frequency per user
user_counts = df.groupby('userID').size()
user_counts.hist(bins=50)
plt.title('Number of Ratings per User')
plt.xlabel('Number of Ratings')
plt.ylabel('Frequency')
plt.show()



#ratings destribution per book
book_counts = df.groupby('canonical_book_id').size()
book_counts.hist(bins=50)
plt.title('Number of Ratings per Book')
plt.xlabel('Number of Ratings')
plt.ylabel('Frequency')
plt.show()





#avarage rating per book
avg_book_ratings = df.groupby('canonical_book_id')['rating'].mean().reset_index(name='average_rating')



# Merge with the number of ratings per book
avg_book_ratings = pd.merge(avg_book_ratings, book_rating_counts, on='canonical_book_id', how='left')


# Filter books with at least 5 ratings
popular_books = avg_book_ratings[avg_book_ratings['rating_count'] >= 5]


# Sort by average rating
top_rated_books = popular_books.sort_values(by='average_rating', ascending=False).head(10)


# Get book titles
top_rated_books = pd.merge(top_rated_books, df[['canonical_book_id', 'bookTitle']].drop_duplicates(), on='canonical_book_id', how='left')


print(top_rated_books[['bookTitle', 'average_rating', 'rating_count']])





plt.scatter(popular_books['rating_count'], popular_books['average_rating'], alpha=0.5)
plt.title('Average Rating vs. Number of Ratings')
plt.xlabel('Number of Ratings')
plt.ylabel('Average Rating')
plt.show()






df['timestamp'] = pd.to_datetime(df['timestamp'])
df['year'] = df['timestamp'].dt.year


ratings_per_year = df.groupby('year').size().reset_index(name='num_ratings')
plt.plot(ratings_per_year['year'], ratings_per_year['num_ratings'])
plt.title('Number of Ratings Over Time')
plt.xlabel('Year')
plt.ylabel('Number of Ratings')
plt.show()





 user_book_matrix = df.pivot_table(index='userID', columns='bookID', values='rating')


import seaborn as sns

# Select a subset for visualization (e.g., first 50 users and books)
subset = user_book_matrix.iloc[:50, :50]
sns.heatmap(subset, cmap='YlGnBu')
plt.title('Heatmap of User-Book Ratings (First 50 users and books)')
plt.xlabel('Book ID')
plt.ylabel('User ID')
plt.show()







genre_counts = df['bookCategory'].value_counts().reset_index()
genre_counts.columns = ['category', 'count']
genre_counts.head(10).plot(kind='barh', x='category', y='count', legend=False)
plt.title('Top 10 Categories')
plt.xlabel('Number of Ratings')
plt.ylabel('Category')
plt.gca().invert_yaxis()
plt.show()






from wordcloud import WordCloud

text = ' '.join(df['bookTitle'].dropna().tolist())
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(15, 7.5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Book Titles')
plt.show()






avg_user_ratings = df.groupby('userID')['rating'].mean().reset_index(name='average_rating')
avg_user_ratings['average_rating'].hist(bins=50)
plt.title('Distribution of Average User Ratings')
plt.xlabel('Average Rating')
plt.ylabel('Number of Users')
plt.show()






df.head(2)


import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import re

nlp = spacy.load('en_core_web_sm')

# Function to expand common abbreviations in titles, publishers, etc.
def expand_abbreviation(text):
    abbreviations = {

        "intl": "international",
        "inc": "incorporated",
        "ltd": "limited",
        "co": "company",
        "sci-fi": "science fiction",

    }
    for abbr, expanded in abbreviations.items():
        text = re.sub(r'\b' + re.escape(abbr) + r'\b', expanded, text)
    return text

# Define separate cleaning functions for each column type
def clean_description(text):
    if not isinstance(text, str):
        return ''
    # Lowercase conversion
    text = text.lower()
    # Tokenize and remove stop words, punctuation, special characters
    doc = nlp(text)
    tokens = [
        token.lemma_ for token in doc
        if not token.is_stop and not token.is_punct and not token.like_num and len(token) > 2
    ]
    cleaned_text = ' '.join(tokens)
    return cleaned_text

def clean_publisher(text):
    if not isinstance(text, str):
        return ''
    # Lowercase and expand abbreviations
    text = text.lower()
    text = expand_abbreviation(text)
    # Tokenize and remove stop words, punctuation, special characters
    doc = nlp(text)
    tokens = [
        token.lemma_ for token in doc
        if not token.is_punct and not token.like_num
    ]
    cleaned_text = ' '.join(tokens)
    return cleaned_text

def clean_categories(text):
    if not isinstance(text, str):
        return ''
    # Split categories based on delimiter and clean whitespace
    tokens = text.split(",")  # Assume categories are comma-separated
    tokens = [token.strip().lower() for token in tokens if token.strip()]
    # Optionally, you can further process tokens (e.g., lemmatization)
    cleaned_text = ' '.join(tokens)
    return cleaned_text

def clean_authors(text):
    if not isinstance(text, str):
        return ''
    # Lowercase and expand abbreviations if needed
    text = text.lower()
    text = expand_abbreviation(text)
    # Tokenize and remove stop words, punctuation, special characters
    doc = nlp(text)
    tokens = [
        token.lemma_ for token in doc
        if not token.is_punct and not token.like_num
    ]
    cleaned_text = ' '.join(tokens)
    return cleaned_text

# Apply column-specific cleaning functions
df['description'] = df['description'].apply(clean_description)
df['publisher'] = df['publisher'].apply(clean_publisher)
df['categories'] = df['categories'].apply(clean_categories)
df['authors'] = df['authors'].apply(clean_authors)

# Example of how the dataset could be cleaned for tokenization and text normalization
df.head(2)  # This will show the cleaned dataset for review




columns_to_combine = ['description', 'Title', 'authors', 'publisher', 'categories']

# Combine the columns into a new 'combined_text' column
df['tags'] = df[columns_to_combine].agg(' '.join, axis=1)

# Remove extra whitespace
df['tags'] = df['tags'].str.replace('\s+', ' ', regex=True).str.strip()

# Display the DataFrame to verify
print(df[['tags']].head())






df = pd.read_csv('newwww.csv')


df.columns





# Calculate the average rating for each book
#avrg_ratings = df.groupby('bookID')['rating'].mean()

# Merge the results back to the original dataframe
#df = df.merge(avrg_ratings.rename('average_ratings'), on='bookID', how='left')


# Calculate the overall average rating for all books (C)
C = df['average_ratings'].mean()
print(C)




# Set the minimum number of ratings required to be considered (m)
m = df['rating_count'].quantile(0.90)  # For example, taking the top 10% as the threshold


# Apply the weighted rating formula to get the weighted score (WR)
#df['weighted_rating'] = (df['rating_count'] / (df['rating_count'] + m)) * df['average_ratings'] + (m / (df['rating_count'] + m)) * C

# Sort the books by weighted rating and drop duplicates
rating_based_recommendations = df.sort_values(by='weighted_rating', ascending=False).drop_duplicates(subset='bookID').head(10)
# Select and display the relevant columns
final_columns = ['bookTitle','bookAuthor','bookCategory', 'Price', 'weighted_rating', 'image' ]
rating_based_recommendations= rating_based_recommendations[final_columns]






print("Rating Base Recommendation System: ")
rating_based_recommendations.head(10)


highest_rated = rating_based_recommendations.to_csv("highest_rated.csv", index=False)





df.columns


dfc=df.copy()


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import string
from nltk.corpus import stopwords
import nltk

nltk.download('stopwords')


# Compute TF-IDF and cosine similarity
def compute_similarity(text_data):
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix_content = tfidf_vectorizer.fit_transform(text_data)
    cosine_similarities_content = cosine_similarity(tfidf_matrix_content, tfidf_matrix_content)
    return cosine_similarities_content


cosine_similarities_content = compute_similarity(df['tags'])


cosine_similarities_content


stop_words = set(stopwords.words('english'))
# Text preprocessing function
def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove stopwords
    text = ' '.join([word for word in text.split() if word not in stop_words])
    return text


def contentBased_recommendations(title, cosine_similarities, df, top_n=10):
    # Preprocess the input title
    input_title = preprocess_text(title)


    if not input_title:
        print(f"Title '{title}' not found in the dataset.")
        return pd.DataFrame()

    # Create a temporary column for processed titles
    df['processed_title'] = df['bookTitle'].apply(preprocess_text)

    # Find the index of the input title
    idx_list = df[df['processed_title'] == input_title].index.tolist()
    if not idx_list:
        print(f"Title '{title}' not found in the dataset.")
        df.drop('processed_title', axis=1, inplace=True)
        return pd.DataFrame()

    # use the first occurrence
    idx = idx_list[0]

    # Get similarity scores for all books
    sim_scores = list(enumerate(cosine_similarities[idx]))

    # Exclude the book itself and any other books with the same normalized title
    sim_scores = [score for score in sim_scores if score[0] not in idx_list]

    # Sort the books based on similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

   # Keep track of unique book titles
    unique_titles = set()
    filtered_sim_scores = []
    for i, score in sim_scores:
        book_title = df.iloc[i]['Title']
        if book_title not in unique_titles:
            unique_titles.add(book_title)
            filtered_sim_scores.append((i, score))
        # Stop if we have enough recommendations
        if len(filtered_sim_scores) >= top_n:
            break

    # Get the indices and scores of the top_n unique similar books
    book_indices = [i for i, _ in filtered_sim_scores]
    similarity_scores = [score for _, score in filtered_sim_scores]

    # Get the recommended books with their scores
    recommendations = df.iloc[book_indices].copy()
    recommendations['score_content'] = similarity_scores

    # Drop the temporary column
    df.drop('processed_title', axis=1, inplace=True)

    return recommendations







# Get recommendations as a DataFrame
title = 'The Hobbit'  # Replace with an actual title from your dataset
recommendations_df = contentBased_recommendations(title, cosine_similarities_content, df)

# Select desired columns
desired_columns = ['bookID','bookTitle','bookAuthor','bookCategory', 'Price', 'weighted_rating', 'image']
recommendations_df = recommendations_df[desired_columns]

# Check if recommendations are found
if recommendations_df.empty:
    print(f"No recommendations found for '{title}'.")
else:
    # Display the DataFrame
    print(f"Recommendations for '{title}':")
      # Display up to 10 recommendations
    print(recommendations_df.head(10).to_string())






df.columns


from surprise import SVD, Dataset, Reader

def collaborative_filtering_recommendations(df, target_user_id, top_n=10):
    # Prepare the data
    reader = Reader(rating_scale=(df['rating'].min(), df['rating'].max()))
    data = Dataset.load_from_df(df[['userID', 'bookID', 'rating']], reader)
    trainset = data.build_full_trainset()

    # Build the model
    algo = SVD()
    algo.fit(trainset)

    # Get a list of all book IDs
    all_book_ids = df['bookID'].unique()

    # Predict ratings for all books not rated by the user
    user_rated_books = df[df['userID'] == target_user_id]['bookID'].unique()
    books_to_predict = [iid for iid in all_book_ids if iid not in user_rated_books]

    predictions = [algo.predict(target_user_id, iid) for iid in books_to_predict]

    # Get top N recommendations
    predictions.sort(key=lambda x: x.est, reverse=True)
    top_predictions = predictions[:top_n]
    top_book_ids = [pred.iid for pred in top_predictions]
    scores = [pred.est for pred in top_predictions]

    # Get book details
    recommendations = df[df['bookID'].isin(top_book_ids)].drop_duplicates('bookID')
    recommendations = recommendations[['bookID','bookTitle','bookAuthor','bookCategory', 'Price', 'weighted_rating', 'image']].copy()
    recommendations['score_collaborative'] = scores

    return recommendations









from surprise import Dataset, Reader, SVD
from surprise.model_selection import cross_validate, KFold
from collections import defaultdict

# Load data
reader = Reader(rating_scale=(df['rating'].min(), df['rating'].max()))
data = Dataset.load_from_df(df[['userID', 'bookID', 'rating']], reader)

# Define algorithm
algo = SVD()

# Step 3: Evaluate with RMSE and MAE using cross-validation
cv_results = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)
print("Average RMSE:", cv_results['test_rmse'].mean())
print("Average MAE:", cv_results['test_mae'].mean())

# Step 4: Compute Precision@K and Recall@K
def precision_recall_at_k(predictions, k=10, threshold=4.0):
    '''Return precision and recall at k metrics for each user.'''

    # Map the predictions to each user.
    user_est_true = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = dict()
    recalls = dict()
    for uid, user_ratings in user_est_true.items():

        # Sort user ratings by estimated value
        user_ratings.sort(key=lambda x: x[0], reverse=True)

        # Number of relevant items
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)

        # Number of recommended items in top k
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])

        # Number of relevant and recommended items in top k
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))
                              for (est, true_r) in user_ratings[:k])

        # Precision@K: Proportion of recommended items that are relevant
        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0

        # Recall@K: Proportion of relevant items that are recommended
        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0

    return precisions, recalls

kf = KFold(n_splits=5)

precisions_list = []
recalls_list = []

for trainset, testset in kf.split(data):
    # Train the algorithm
    algo.fit(trainset)

    # Test the algorithm
    predictions = algo.test(testset)

    # Compute precision and recall
    precisions, recalls = precision_recall_at_k(predictions, k=10, threshold=4.0)

    # Average precision and recall for this fold
    precision = sum(prec for prec in precisions.values()) / len(precisions)
    recall = sum(rec for rec in recalls.values()) / len(recalls)

    precisions_list.append(precision)
    recalls_list.append(recall)

# Compute overall average precision and recall
avg_precision = sum(precisions_list) / len(precisions_list)
avg_recall = sum(recalls_list) / len(recalls_list)

print(f'Average Precision@10: {avg_precision:.4f}')
print(f'Average Recall@10: {avg_recall:.4f}')






target_user_id = 'AZSV99SDJC242'
top_n = 10
recommendations = collaborative_filtering_recommendations(df, target_user_id, top_n=top_n)
print(f"Top {top_n} recommendations for user {target_user_id}:")
recommendations





df.columns



